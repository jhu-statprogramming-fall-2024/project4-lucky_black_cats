---
title: "project4"
format: html
editor: visual
---

## Project 4: Using Wikipedia Data to Predict Stock Prizes

Team Members:

-   Mateo Bandala Jacques \| abandal1\@jh.edu

-   María Camila Restrepo \| mrestre\@jh.edu

-   Mitali Joshi \| mjoshi13\@jh.edu

# Setup

```{r}
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("httr")) install.packages("httr")
if (!require("jsonlite")) install.packages("jsonlite")
if (!require("quantmod")) install.packages("quantmod")
if (!require("dplyr")) install.packages("dplyr")
if (!require("caret")) install.packages("caret")

library(httr)
library(jsonlite)
library(quantmod)
library(dplyr)
library(caret)
library(tidyverse)



```

::: callout-note
## First, we will build and validate the predictive model that will go in our user interface
:::

# Part 1: Train and test a machine learning algorithm to predict stock prizes

## Set-up the functions to extract the data

```{r}

# First, we will create a function to get the wikipedia page views

get_wiki_pageviews <- function(page, start_date, end_date) {                # Input page and dates
  page <- gsub(" ", "_", page)  # Fix the URL name
  url <- paste0(
    "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/",
    "en.wikipedia/all-access/all-agents/", page, "/daily/",
    format(as.Date(start_date), "%Y%m%d"), "/", format(as.Date(end_date), "%Y%m%d")
  )
  
  response <- GET(url)
  
  if (status_code(response) == 200) {      #200 is the succesful response
    # Parse the JSON response
    parsed_data <- fromJSON(content(response, "text", encoding = "UTF-8"))   #Encoding 
    
    # This extracts a datframe
    items <- parsed_data$items
    
    # Some data wrangling from the above step
    df <- items %>%
      dplyr::mutate(
        Date = as.Date(substr(timestamp, 1, 8), "%Y%m%d")  # convert to date for the dataframe
      ) %>%
      dplyr::select(Date, views) %>%
      dplyr::rename(Pageviews = views)  # 
    
    return(df)
  } else {
    stop("Failed to retrieve Wikipedia page views. HTTP status: ", status_code(response))
  }
}


#Simple tests

get_wiki_pageviews("Tesla, Inc.", "2024-09-01" ,"2024-09-06")
get_wiki_pageviews("Donald Trump", "2024-09-01" ,"2024-09-06")
get_wiki_pageviews("Chihuahua_(state)", "2024-09-01" ,"2024-09-06") #Mateo's home state :)
get_wiki_pageviews("Medellín", "2024-09-01" ,"2024-09-06") #Camila's home city, which of course has more views than Chihuaua



# Now pull the stock (ticker) data from YAHOO
get_stock_data <- function(ticker, start_date, end_date) {
  stock_data <- getSymbols(
    ticker, src = "yahoo", from = as.Date(start_date), to = as.Date(end_date),
    auto.assign = FALSE
  )
  stock_df <- data.frame(
    Date = index(stock_data),
    Stock_Close = as.numeric(Cl(stock_data))
  )
  return(stock_df)
}


#Some more simple tests for this

get_stock_data("TSLA", "2024-09-01" ,"2024-09-06")
get_stock_data("AAPL", "2024-09-01" ,"2024-09-06")
get_stock_data("BBVA", "2024-09-01" ,"2024-09-06")  #This works


```

## Prepare the data

```{r}


#This will take in the data created above
prepare_ml_data <- function(data) {
  # Lag the pageviews (or else we will predict the past)
  for (i in 1:7) {   #I think 7 days are enough
    data[[paste0("Pageviews_lag", i)]] <- dplyr::lag(data$Pageviews, n = i)
  }
  
  # Outcome will be 1 = UP up, 0 = DOWN or no change
  data$Price_Change <- ifelse(dplyr::lead(data$Stock_Close) > data$Stock_Close, 1, 0)
  
  # Remove rows with NA values
  data <- na.omit(data)
  
  return(data)
}



#Let's test it with our data

wiki_data <- get_wiki_pageviews("Tesla, Inc.", "2024-01-01", "2024-03-01") #Make sure this is at least TWO months! (see below)
stock_data <- get_stock_data("TSLA", "2024-01-01", "2024-03-01")
combined_data <- merge(wiki_data, stock_data, by = "Date")

# Apply prepare_ml_data to real data
prepared_real_data <- prepare_ml_data(combined_data)
print(prepared_real_data)


```

## Train and test the model.

```{r}
 
train_and_test <- function(data) {
  # Use all rows except the last for training (I had to increase this or else the model fails!)
  train_data <- data[1:(nrow(data) - 1), ]   
  test_data <- data[nrow(data), ]  # The last row is the test set to test it
  
  # Force the outcome to factor (this crashed it earlier)
  train_data$Price_Change <- as.factor(train_data$Price_Change)
  
  # Train logistic regression model
  model <- train(
    Price_Change ~ ., 
    data = train_data[, c("Price_Change", paste0("Pageviews_lag", 1:7))],  #These are the VARIABLES, do NOT change to 1:nrow
    method = "glm", 
    family = "binomial",
    trControl = trainControl(method = "none")  # Disable cross-validation or it crashes
  )
  
  # Predict for the test day (last now)
  prediction <- predict(model, newdata = test_data[, paste0("Pageviews_lag", 1:7), drop = FALSE])
  
  # Return the prediction and actual value
  return(list(
    prediction = prediction,
    actual = test_data$Price_Change
  ))
}

train_and_test(prepared_real_data) #Wow! actually actually predicted :)

```

## Run all the prior for some Wiki pages and tickers, and see how well it performs

::: callout-note
## Do NOT run the next part unless necessary, as it makes TEN requests to each of the APIs
:::


```{r}
stocks <- c("TSLA", "AAPL", "MSFT", "GOOGL", "AMZN", "META", "NFLX", "NVDA", "BRK-B", "JNJ")
wiki_pages <- c("Tesla, Inc.", "Apple_Inc.", "Microsoft", "Alphabet_Inc.", "Amazon_(company)", "Meta_Platforms", "Netflix", "Nvidia", "Berkshire_Hathaway", "Johnson_%26_Johnson")
start_date <- "2024-03-01"
end_date <- "2024-05-01"

# Loop through associations
results <- lapply(1:length(stocks), function(i) {
  stock <- stocks[i]
  wiki_page <- wiki_pages[i]
  
  # Pull data
  wiki_data <- get_wiki_pageviews(wiki_page, start_date, end_date)
  stock_data <- get_stock_data(stock, start_date, end_date)
  
  # Merge data
  combined_data <- merge(wiki_data, stock_data, by = "Date")
  
  # Prepare data
  prepared_data <- prepare_ml_data(combined_data)
  
  # Train and test
  train_and_test(prepared_data)
})

# Print results
results

# The model worked on 6/10


```

# Part 2

```{r}
# Define UI for the Shiny app
# I HAVE AN EXTRA PARTENTHESIS SOMEWHERE AHHHH

ui <- fluidPage(      #For a responsive web layout
  titlePanel("Wikipedia & Stock Price Correlation"),     #This goes at the top of the app
  
  #This is where the user will input Wiki and stock -  For now let's stick to TESLA
  sidebarLayout(
    sidebarPanel(
      textInput("stock", "Enter Stock Ticker (e.g., TSLA)", "TSLA"),
      textInput("wiki_page", "Enter Wikipedia Page (e.g., Tesla, Inc.)", "Tesla, Inc."),
      dateRangeInput("date_range", "Select Date Range",
                     start = "2024-01-01", end = Sys.Date())
    ),
    
    #This is  the output
    mainPanel(
      plotOutput("plot"),
      textOutput("error"),
      
      # New output for model summary and prediction results
      verbatimTextOutput("model_summary"),  #  For the caret prediction model
      textOutput("prediction")               # Either UP or DOWN
    )
  )
)





```
